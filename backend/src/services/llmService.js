import { HfInference } from '@huggingface/inference';
import dotenv from 'dotenv';

dotenv.config();

class LlmService {
  constructor() {
    this.hf = new HfInference(process.env.HUGGINGFACE_API_KEY);

    // S·ª≠ d·ª•ng Qwen2.5-7B-Instruct - Model free t·ªët nh·∫•t cho ti·∫øng Vi·ªát
    this.model = 'Qwen/Qwen2.5-7B-Instruct';

    this.systemPrompt = `B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng laptop th√¥ng minh c·ªßa m·ªôt c·ª≠a h√†ng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠.

NHI·ªÜM V·ª§:
1. T∆∞ v·∫•n v√† gi·ªõi thi·ªáu s·∫£n ph·∫©m laptop d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
2. Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ th√¥ng s·ªë k·ªπ thu·∫≠t, gi√° c·∫£, so s√°nh s·∫£n ph·∫©m
3. G·ª£i √Ω s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu v√† ng√¢n s√°ch kh√°ch h√†ng
4. Gi·∫£i th√≠ch c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t m·ªôt c√°ch d·ªÖ hi·ªÉu

QUY T·∫ÆC QUAN TR·ªåNG:
- LU√îN d·ª±a v√†o TH√îNG TIN S·∫¢N PH·∫®M ƒë∆∞·ª£c cung c·∫•p trong context
- KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c th√™m th√¥ng s·ªë k·ªπ thu·∫≠t kh√¥ng c√≥ trong d·ªØ li·ªáu
- KH√îNG ƒë·ªÅ c·∫≠p s·∫£n ph·∫©m kh√¥ng c√≥ trong th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng, d·ªÖ hi·ªÉu
- Lu√¥n ƒë·ªÅ c·∫≠p T√äN S·∫¢N PH·∫®M v√† GI√Å C·ª§ TH·ªÇ khi gi·ªõi thi·ªáu
- Gi·∫£i th√≠ch T·∫†I SAO g·ª£i √Ω s·∫£n ph·∫©m ƒë√≥ (∆∞u ƒëi·ªÉm ph√π h·ª£p v·ªõi nhu c·∫ßu)
- S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ l√†m r√µ √Ω (‚≠ê üíª üéÆ üí∞ ‚úÖ ‚ùå)
- N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y th·ª´a nh·∫≠n v√† h·ªèi th√™m
- Khi gi·ªõi thi·ªáu s·∫£n ph·∫©m, ƒë·ªÅ c·∫≠p r·∫±ng kh√°ch h√†ng c√≥ th·ªÉ click v√†o card s·∫£n ph·∫©m b√™n d∆∞·ªõi ƒë·ªÉ xem chi ti·∫øt

C√ÅCH TR·∫¢ L·ªúI:
- V·ªõi c√¢u h·ªèi chung: G·ª£i √Ω 2-3 s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t
- V·ªõi c√¢u h·ªèi c·ª• th·ªÉ: Tr·∫£ l·ªùi tr·ª±c ti·∫øp d·ª±a v√†o th√¥ng tin
- V·ªõi so s√°nh: ƒê∆∞a ra b·∫£ng so s√°nh ho·∫∑c ph√¢n t√≠ch t·ª´ng ƒëi·ªÉm
- Cu·ªëi m·ªói c√¢u tr·∫£ l·ªùi: H·ªèi th√™m ƒë·ªÉ hi·ªÉu r√µ nhu c·∫ßu
- Lu√¥n nh·∫Øc kh√°ch "Click v√†o s·∫£n ph·∫©m b√™n d∆∞·ªõi ƒë·ªÉ xem ƒë·∫ßy ƒë·ªß th√¥ng tin v√† ƒë·∫∑t h√†ng"`;
  }

  /**
   * Generate response t·ª´ LLM v·ªõi context t·ª´ RAG
   */
  async generateResponse(userMessage, context, conversationHistory = []) {
    const startTime = Date.now();

    try {
      // Build messages
      const messages = this.buildMessages(userMessage, context, conversationHistory);

      console.log('ü§ñ Generating response with LLM...');

      // Call HuggingFace Inference API
      let fullResponse = '';

      const stream = await this.hf.chatCompletionStream({
        model: this.model,
        messages: messages,
        max_tokens: 800,
        temperature: 0.7,
        top_p: 0.9
      });

      // Collect streamed response
      for await (const chunk of stream) {
        if (chunk.choices && chunk.choices.length > 0) {
          const delta = chunk.choices[0].delta;
          if (delta.content) {
            fullResponse += delta.content;
          }
        }
      }

      const generationTime = Date.now() - startTime;
      console.log(`‚úÖ LLM response generated in ${generationTime}ms`);

      // Estimate tokens (rough approximation)
      const estimatedTokens = Math.ceil(fullResponse.length / 4);

      return {
        content: fullResponse.trim(),
        model: this.model,
        generation_time_ms: generationTime,
        tokens_used: estimatedTokens
      };
    } catch (error) {
      console.error('‚ùå LLM generation error:', error);

      // Fallback response n·∫øu LLM fail
      return {
        content: this.generateFallbackResponse(context),
        model: 'fallback',
        generation_time_ms: Date.now() - startTime,
        tokens_used: 0,
        error: error.message
      };
    }
  }

  /**
   * Build messages array cho LLM
   */
  buildMessages(userMessage, context, conversationHistory) {
    const messages = [
      {
        role: 'system',
        content: this.systemPrompt
      }
    ];

    // Add conversation history (last 5 messages)
    const recentHistory = conversationHistory.slice(-5);
    recentHistory.forEach((msg) => {
      messages.push({
        role: msg.role,
        content: msg.content
      });
    });

    // Add current user message v·ªõi context
    messages.push({
      role: 'user',
      content: `${context}\n\n---\n\nC√¢u h·ªèi c·ªßa kh√°ch h√†ng: ${userMessage}`
    });

    return messages;
  }

  /**
   * Generate fallback response khi LLM fail
   */
  generateFallbackResponse(context) {
    if (context.includes('Kh√¥ng t√¨m th·∫•y')) {
      return 'Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n v·ªÅ nhu c·∫ßu s·ª≠ d·ª•ng v√† ng√¢n s√°ch c·ªßa m√¨nh kh√¥ng?';
    }

    // Extract product info from context (simple parsing)
    const productMatches = context.match(/\[S·∫£n ph·∫©m \d+\] (.+)\nGi√°: (.+)/g);

    if (productMatches && productMatches.length > 0) {
      let response = 'D·ª±a v√†o y√™u c·∫ßu c·ªßa b·∫°n, t√¥i t√¨m th·∫•y c√°c s·∫£n ph·∫©m sau:\n\n';

      productMatches.forEach((match, idx) => {
        const nameMatch = match.match(/\[S·∫£n ph·∫©m \d+\] (.+)/);
        const priceMatch = match.match(/Gi√°: (.+)/);

        if (nameMatch && priceMatch) {
          response += `${idx + 1}. ${nameMatch[1]} - ${priceMatch[1]}\n`;
        }
      });

      response += '\nB·∫°n mu·ªën bi·∫øt th√™m th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m n√†o?';
      return response;
    }

    return 'Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t ho·∫∑c li√™n h·ªá b·ªô ph·∫≠n h·ªó tr·ª£.';
  }

  /**
   * Validate API key
   */
  validateApiKey() {
    if (!process.env.HUGGINGFACE_API_KEY) {
      throw new Error('HUGGINGFACE_API_KEY is not configured in .env file');
    }
    return true;
  }
}

export default new LlmService();
